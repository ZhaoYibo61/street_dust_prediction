{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fKabTvqRIThK"
   },
   "source": [
    "# Predicting street dust concentrations with dilated causal convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P18xFrJJIEVI"
   },
   "source": [
    "## Problem and data description\n",
    "\n",
    "A model for predicting street dust concentrations for 48 hours ahead is developed in this project work. Model architecture has got inspiration from adapted WaveNet network by *Borovykh et al., 2018* . Network contains stacks of dilated convolutions that allow it to access a broad range of history when forecasting, residual connections, a SELU activation function and conditioning with multivariate input data. PyTorch is used to build the model.\n",
    "\n",
    "Street dust is a recurring problem in Finland especially in springtime because sand and other road surface material are released into the air when snow and ice have melted and streets have dried. Street dust consist of materials from street surfaces, sanding materials, and materials from brakes and studded tires. Street dust concentrations are typically higher also during end of the year when road sanding usually begins, but roads are not yet fully covered by snow and ice. Construction sites and street work also cause street dust. Most street dust particles are less than 10 micrometres (PM10) in diameter. \n",
    "\n",
    "Street dust can cause respiratory tract irritation causing cold-like symptoms, coughing and irritation to the eyes, nose and throat. People suffering from asthma, small children and and those suffering from the coronary artery disease are most vulnerable to adversarial effects of street dust. Weather conditions,  quality of the road surface and amount of traffic affects street dust concentration levels. Wet conditions prevent dust from street surfaces rising into the air, and gusty wind can lift street dust efficiently from unclean roads. The dustiest areas are busy streets and cross-roads during rush hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oFmlGkKLif_Z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data is history observations of hourly PM10 concentrations, amount of rain, temperature and wind speed. Concentration data is from air quality monitoring station located at near busy street (Mäkelänkatu) in Helsinki, and weather data is from nearby weather station (Kumpula). Data is from period 01.01.2017-30.4.2019, i.e. 28 months of hourly observations. Data is downloaded from https://en.ilmatieteenlaitos.fi/download-observations#!/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QSjTQYRBoZ2L"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "eAnv9I39i_lW"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('/content/drive/My Drive/data/PM10_weather_data_LONG2.csv',  parse_dates=[['Year', 'Month', 'Day', 'Time']])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "LEzeY6xtqI4A"
   },
   "outputs": [],
   "source": [
    "# print length of data and start and end time\n",
    "data_size = df.shape[0]\n",
    "print(\"Total amount of hours:\", data_size)\n",
    "first = df['Year_Month_Day_Time'].iloc[0]\n",
    "last = df['Year_Month_Day_Time'].iloc[-1]\n",
    "print(\"Data ranges from %s to %s.\" % (first, last))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PM10 concentration values varies between -5 and 280 micrograms/m^3. Negative values are probably due to some malfunction in measurement device. From the histograms at Figure 1 we can see that distributions of PM10 concentration and amount of rain are skewed. Distribution of temperature and wind speed are more closer to normal distribution. Time series of PM10 concentration and temperature are shown in Figure 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "id": "8a4s3fmyFcTi",
    "outputId": "bfc2e5c1-01de-476d-c487-abe8e7b0c8f1"
   },
   "outputs": [],
   "source": [
    "# plot histograms of input data\n",
    "plt.rcParams['figure.figsize']=(18,6)\n",
    "fig3, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)\n",
    "ax1.hist(df_fill['PM10 concentration (ug/m3)'], 50)\n",
    "ax2.hist(df_fill['Amount of rain (mm/h)'], 10)\n",
    "ax3.hist(df_fill['Temperature (degC)'], 40)\n",
    "ax4.hist(df_fill['Wind speed (m/s)'], 40)\n",
    "ax1.set_xlabel('PM10 concentration (ug/m3)')\n",
    "ax2.set_xlabel('Amount of rain (mm/h)')\n",
    "ax3.set_xlabel('Temperature (degC)')\n",
    "ax4.set_xlabel('Wind speed (m/s)')\n",
    "fig3.savefig('hist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hist.png\" width=1000 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1. Histograms of input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EXS1WbKzBiBx"
   },
   "outputs": [],
   "source": [
    "# plot temperature and PM10 concentration\n",
    "plt.rcParams['figure.figsize']=(18,8)\n",
    "fig2, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx() \n",
    "ax1.plot(df_fill['Temperature (degC)'], color='orange', label='Temperature')\n",
    "ax2.plot(df_fill['PM10 concentration (ug/m3)'], label='PM10 concentration')\n",
    "ax1.set_ylabel('Temperature (degC)')\n",
    "ax2.set_ylabel('PM10 concentration (ug/m3)')\n",
    "ax2.set_ylim(0, 250)\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.margins(x=0,y=0)\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_title('PM10 concentration and temperature')\n",
    "fig2.savefig('temp_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"temp_plot.png\" width=1000 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2. Time series of PM10 concentration and temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2KfEMRx4Li_i"
   },
   "outputs": [],
   "source": [
    "# plot wind speed and PM10 concentration\n",
    "plt.rcParams['figure.figsize']=(18,8)\n",
    "fig2, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx() \n",
    "ax1.plot(df_fill['Wind speed (m/s)'], color='orange', label='Wind speed')\n",
    "ax2.plot(df_fill['PM10 concentration (ug/m3)'], label='PM10 concentration')\n",
    "ax1.set_ylabel('Wind speed (m/s)')\n",
    "ax2.set_ylabel('PM10 concentration (ug/m3)')\n",
    "ax2.set_ylim(0, 250)\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.margins(x=0,y=0)\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_title('PM10 concentration and wind speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xIntKo9WcYVB"
   },
   "outputs": [],
   "source": [
    "# plot amount of rain and PM10 concentration\n",
    "plt.rcParams['figure.figsize']=(18,8)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx() \n",
    "ax1.bar(df_fill['Amount of rain (mm/h)'].index, df_fill['Amount of rain (mm/h)'], alpha=0.7, color='orange', label='Amount of rain')\n",
    "ax2.plot(df_fill['PM10 concentration (ug/m3)'], label='PM10 concentration')\n",
    "ax1.set_ylabel('Amount of rain (mm/h)')\n",
    "ax2.set_ylabel('PM10 concentration (ug/m3)')\n",
    "ax2.set_ylim(0, 250)\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.margins(x=0,y=0)\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_title('PM10 concentration and amount of rain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_bC0nuSKS-C"
   },
   "source": [
    "## Method\n",
    "\n",
    "The adopted dilated causal convolutional neural network stucture is based on Google's famous WaveNet model from first developed for audio forecasting (*van den Oord et al., 2016*).  The intuition behind using convolutional neural networks (CNN) for time series forecasting is that CNNs should be able to learn filters that represent certain repeating patterns in the time series, similarly as with images,  and use these to forecast the future values. With timeseries we use one dimensional convolution. Training a model on multivariate input data allows the network to exploit the correlation structure between these multiple time series and learning long-term temporal dependencies in between series . The advantage of the CNN over the recurrent-type network is that due to the convolutional structure of the network, the number of trainable weights is small, resulting in a much more efficient training and predicting *(Borovykh et al., 2018)*.\n",
    "\n",
    "In a dilated convolution the filter is applied to every \"dilation rate\"th element in the input vector, allowing the model to efficiently learn connections between far-apart data points. Using the dilated convolutions instead of regular ones allows the output to be influenced by more nodes in the input i.e. expand the receptive field of the model.  The receptive field of the model is the number of neurons (time steps) in the input that can modify the output in the final layer, i.e. the forecasted time series. The receptive field depends on the number of layers L and the filter size k. In this model architecture the dilations are increasing in every layer by a factor of two: 1, 2, 4, 8, etc. The filter size (same as kernel size) is chosen to be 2 and number of layers is 9, so the receptive field of the model is 512 hours (3 weeks) in this experiment. Figure 3 represents illustration of dilated layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dilate.png\" width=1000 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3. Illustration of dilation layers (from https://github.com/JEddy92/TimeSeries_Seq2Seq/blob/master/notebooks/TS_Seq2Seq_Conv_Full.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word causal indicates that the convolution output should not depend on future inputs. In time series this is equivalent to padding the input (in the left) with a vector of zeros of the size of the receptive field. Conditioning with multiple input time series is done with separate convolutions with SELU (scaled exponential linear unit) activation function and skip connections for each input series and then concatenated together to form output from the first layer. In the first layer dilation rate is 1. The skip connections are parametrized by 1×1 convolutions, i.e. kernel size is 1, and number of output channels (filters) of 1. If some condition does not improve the forecast, the model can simply learn to discard this condition by setting the weights in the parametrized skip connection to zero. In the first layer each input time series skip connections and convolution outputs are concatenated together and then they are passed to 1×1 convolution to form the output of first layer. Subsequent layers have convolution with increasing dilation rate and SELU activation function and residual connection from input to output of the layer. Residual connection is added to the convolution output to form the final output of the layer. After the last layer there is final 1×1 convolution which outputs the forecasted time series. Only last 48 hours are taken from the final output.\n",
    "\n",
    "Figure 4 illustrates the model architecture of Borovykh et al., 2018. In our adopted version separate skip connections and convolution outputs from the input time series are concatenated not summed. Both methods are commonly used in adding residual/skip connections to the convolution output, summing and concatenating. We decided to use latter method in hope that the information from conditioning input time series would propagate better in the network. We have also extra 1×1 convolution in the first layer before the output to decrease the number of filters after concatenation operation. We tried also with different activation functions, namely ReLU and LeakyReLU, the architecture presented here is with SELU activations. SELU was chosen in first place because it can handle also negative values. We didn't notice clear difference in performance with these different activation functions. Number of filters were chosen to be quite small in the convolutional layers to avoid overfitting with our quite small dataset. Hidden size of convolutional layers was set to 3. We tried also bigger values but those yielded overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Augmented_wavenet.png\" width=1000 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4. Model architecture of augmented Wavenet by Borovykh et al., 2018 (figure taken from the article)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the model structure which is used in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UBMaADy6GQkr"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, kernel_size, dilation_rate, bias=False):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.hidden_size = hidden_size,\n",
    "        self.kernel_size = kernel_size,\n",
    "        self.dilation_rate = dilation_rate\n",
    "          \n",
    "        self.conv = nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel_size, dilation=dilation_rate)\n",
    "        self.relu = nn.SELU()\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = inputs\n",
    "        pads = (self.kernel_size[0] - 1) * self.dilation_rate\n",
    "        inputs_padded = F.pad(inputs, (pads, 0))\n",
    "        \n",
    "        layer_out = self.relu(self.conv(inputs_padded))\n",
    "        network_out = torch.add(residual, layer_out)\n",
    "        return network_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "thbEVUfOGTAh"
   },
   "outputs": [],
   "source": [
    "class DilatedConvNet(nn.Module):\n",
    "    def __init__(self, n_layers, num_inputs, hidden_size, kernel_size, dilation_rate=1, bias=False):\n",
    "        super(DilatedConvNet, self).__init__()\n",
    "        self.n_layers = n_layers,\n",
    "        self.hidden_size = hidden_size,\n",
    "        self.kernel_size = kernel_size,\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.num_inputs = num_inputs\n",
    "        \n",
    "        # first layer\n",
    "        self.conv1 = nn.Conv1d(1, hidden_size, kernel_size=kernel_size, dilation=dilation_rate)\n",
    "        self.relu1 = nn.SELU()\n",
    "        self.skip_first = nn.Conv1d(1, 1, kernel_size=1, dilation=dilation_rate)\n",
    "        # decrease the number of filters to hidden size\n",
    "        self.conv2 = nn.Conv1d((hidden_size+1)*num_inputs, hidden_size, kernel_size=1, dilation=dilation_rate)\n",
    "        \n",
    "        # other layers\n",
    "        other_layers = [ConvBlock(hidden_size=hidden_size, kernel_size=kernel_size, dilation_rate=2 ** i) for i in range(1, n_layers)]\n",
    "        self.group = nn.Sequential(*other_layers)\n",
    "        \n",
    "        # last layer\n",
    "        self.dense = nn.Conv1d(hidden_size, 1, kernel_size=1, dilation=dilation_rate)\n",
    "        \n",
    "        # initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                n = m.kernel_size[0] * m.out_channels\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "                \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # first layer\n",
    "        # padding the input with a vector of zeros of the size of the receptive field\n",
    "        pads = (self.kernel_size[0] - 1) * self.dilation_rate\n",
    "        inputs_padded = F.pad(inputs, (pads, 0))\n",
    "        \n",
    "        input_layer = []\n",
    "        for i in range(self.num_inputs):\n",
    "          layer_out = self.relu1(self.conv1(inputs_padded[:,i,:].view(inputs_padded.shape[0],1,-1)))\n",
    "          skip_out = self.skip_first(inputs[:,i,:].view(inputs.shape[0],1,-1))\n",
    "          out = torch.cat((skip_out, layer_out), dim=1)\n",
    "          input_layer.append(out)\n",
    "        out = torch.cat(input_layer, dim=1)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        # other layers\n",
    "        outputs = self.group(out)\n",
    "        \n",
    "        # last layer\n",
    "        network_out = self.dense(outputs)\n",
    "        outputs = network_out[:,:,-output_steps:] # output only last 48 hours\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layer weights are initialized with zero-mean Gaussian distribution and standard deviation of sqrt(2/z), where z is the total number of trainable parameters in the layer. Objective function was chosen to be mean absolute error and we used Adam gradient descent with learning rate 0.001. L2 regularization, i.e. weight decay, was used to avoid overfitting with regularization rate 0.001. These parameters was chosen to be same asi in (Borovykh et al., 2018). Hyperparameter search was not done.\n",
    "\n",
    "In the chosen model architecture the output is a vector of 48 values, i.e. the model predicts directly concentrations for the next 48 hours. The model is static, it is trained only one time. Other option could have been to implement architecture which have 48 separate models, each predicting value for certain hour. Or model wich will output only next hour, then updated with new input data, and this woul be looped over 48 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JXYQaTYW_NTi"
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "n_layers = 9\n",
    "hidden_size = 3\n",
    "kernel_size = 2\n",
    "num_inputs = 4 # number of input features (number of input time series)\n",
    "model = DilatedConvNet(n_layers=n_layers, num_inputs=num_inputs, hidden_size=hidden_size, kernel_size=kernel_size)\n",
    "print(model.to(device))\n",
    "model.to(device)\n",
    "\n",
    "# using mean absolute error as a criterion\n",
    "criterion = nn.L1Loss()\n",
    "# using Adam optimizer and L2 regularization\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "# learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "# Feed a batch of data from the training data to test the network\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(trainloader)\n",
    "    values, labels = dataiter.next()\n",
    "    values = values.to(device)\n",
    "    print('Shape of the input tensor:', values.shape)\n",
    "\n",
    "    y = model(values)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QxYLfn8xQxM8"
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "There are some missing values in the data,  1% of PM10 concentration observations and less than  1% of weather observations are missing. Missing time stamps are filled using linear interpolation. The amount of missing data is small and because the causal nature of input data, the missing time stamps are decided to fill, not delete. Input data consist of 20 400 hours which is divided to training (18 000 hours) and test (2 400 hours) sets. Training data is separated to equal length sequences of training input (512 hours) and training target (48 hours) data in overlapping moving window format. Target data is non-overlapping between sequences. When the training data is splitted using overlapping moving window method there will be  35 separate sequences, i.e. samples. These are further divided to training (30 samples) and validation (5 samples ) sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "K0cVS44OJENx"
   },
   "outputs": [],
   "source": [
    "# check missing values\n",
    "print(\"Missing values:\\n\")\n",
    "print(df.isnull().sum(), \"\\n\")\n",
    "# put the time as index\n",
    "df = df.set_index('Year_Month_Day_Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Pk8OsYVIIYc3"
   },
   "outputs": [],
   "source": [
    "# check min and max values for the concentration i.e. predictant\n",
    "print(df['PM10 concentration (ug/m3)'].min())\n",
    "print(df['PM10 concentration (ug/m3)'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DD2B6A7CYlig"
   },
   "outputs": [],
   "source": [
    "# use linear interpolation for filling the missing values\n",
    "df_fill = df.interpolate()\n",
    "print(df_fill.isnull().sum(), \"\\n\")\n",
    "print(df_fill.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3TG7qEht1fT4"
   },
   "outputs": [],
   "source": [
    "# check data types\n",
    "df_fill.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZgpmFnPeGCL6"
   },
   "outputs": [],
   "source": [
    "# function to split dataset to training and test sets\n",
    "def split_dataset(data, split_point):\n",
    "    train, test = data[0:split_point], data[split_point:data.shape[0]]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gtbNouniJHbn"
   },
   "outputs": [],
   "source": [
    "# split input data to train and test batches\n",
    "input_data = np.stack((df_fill['PM10 concentration (ug/m3)'].values, df_fill['Amount of rain (mm/h)'].values, df_fill['Temperature (degC)'].values, df_fill['Wind speed (m/s)'].values), axis=-1)\n",
    "print(input_data.shape)\n",
    "train, test = split_dataset(input_data, 18000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-VJH7zf9My86"
   },
   "outputs": [],
   "source": [
    "# check the shapes of train and test sets\n",
    "# and check that last value of train data and first value of test data are different\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(train[-1,0])\n",
    "print(test[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the scales are different in input data, it is standardized by removing the mean and scaling to unit variance. There are lot's of zeros in one of the input time series (amount of rain), so also for this reason the scaling is needed. The scaling is done separately for each split, so that the information from training targets doesn't leak to training inputs. When the trained model is used for prediction the scaling have to be done also for the input data, and of course re-scaling before inputting the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "f0cEn-2CGoYh"
   },
   "outputs": [],
   "source": [
    "# convert training data into batches of equal length sequences of train_inputs and train_targets\n",
    "# option for scaling the data is included\n",
    "# modified function, original from https://machinelearningmastery.com/how-to-develop-convolutional-neural-networks-for-multi-step-time-series-forecasting/\n",
    "def split_equal_sequences(data, n_input, n_out, scaling=False):\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the training data one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end < len(data):\n",
    "            x_input = data[in_start:in_end, :]\n",
    "            y_input = data[in_end:out_end, 0].reshape(n_out,1)\n",
    "        # standardize data by removing the mean and scaling to unit variance\n",
    "        if scaling:\n",
    "            scaler = StandardScaler().fit(x_input)\n",
    "            scaler2 = StandardScaler().fit(x_input[:,0].reshape(-1,1))\n",
    "            x_input = scaler.transform(x_input)\n",
    "            y_input = scaler2.transform(y_input.reshape(-1,1))\n",
    "      \n",
    "        X.append(x_input)\n",
    "        y.append(y_input)\n",
    "\t\t# move along one time step\n",
    "        #in_start += n_input\n",
    "        in_start += 1 #try with overlapping moving window\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NHgF2N3TwzvM"
   },
   "outputs": [],
   "source": [
    "# Length of input data (hours of history data) and lenght of prediction period\n",
    "input_steps = 512 # 3 weeka\n",
    "output_steps = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JMruxzCVZD_G"
   },
   "outputs": [],
   "source": [
    "# split training data to input and target sequences. \n",
    "train_inputs, train_targets = split_equal_sequences(train, input_steps, output_steps, scaling=True)\n",
    "print(train_targets.shape)\n",
    "print(train_inputs.shape)\n",
    "\n",
    "# reshape data to input format for convolution layer [batch, channels, steps] and tranform to tensors.\n",
    "train_inputs = train_inputs.reshape(train_inputs.shape[0], train_inputs.shape[2], -1)\n",
    "train_targets = train_targets.reshape(train_targets.shape[0], train_targets.shape[2], -1)\n",
    "x = torch.tensor(train_inputs, device=device, dtype=torch.float)\n",
    "y = torch.tensor(train_targets, device=device, dtype=torch.float)\n",
    "print(x.size())\n",
    "print(y.size())\n",
    "# combine tensors to dataset\n",
    "dataset = torch.utils.data.TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iLwxPI7REb1n"
   },
   "outputs": [],
   "source": [
    "# divide the training dataset further to training and validation datasets.\n",
    "idx = list(range(len(dataset)))\n",
    "train_idx = idx[:15000]\n",
    "val_idx = idx[15000:]\n",
    "dataset_train = torch.utils.data.Subset(dataset, train_idx)\n",
    "dataset_valid = torch.utils.data.Subset(dataset, val_idx)\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BLZcxE_7KThn"
   },
   "source": [
    "## Experiments and results\n",
    "\n",
    "Model is run in Google colab with GPUs. Different setups were tested with different numbers of layers, kernel size, hidden size and input time steps. Adding a dropout layer at the end of each convolutional layer was also tested. None of these tested versions was remarkably better than other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MzFy23u8ipxs"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6jWPe7mLjQ1V"
   },
   "outputs": [],
   "source": [
    "# load training and validation datasets to torch Dataloader format\n",
    "trainloader = torch.utils.data.DataLoader(dataset_train, batch_size=15, shuffle=True, pin_memory=False)\n",
    "validloader = torch.utils.data.DataLoader(dataset_valid, batch_size=5, shuffle=True, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hDWMkN_d9A__"
   },
   "outputs": [],
   "source": [
    "# train model and monitor training and validation error\n",
    "# function modified from Pytorch tutorial: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs):\n",
    "    since = time.time()\n",
    "    val_loss_history = []\n",
    "    train_loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, targets in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                  outputs = model(inputs)\n",
    "                  loss = criterion(outputs, targets)\n",
    "                  # backward + optimize only if in training phase\n",
    "                  if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if (epoch % 10) == 0:\n",
    "              print('Epoch {}/{} {} loss: {:.4f}'.format(epoch, num_epochs - 1, phase, epoch_loss))\n",
    "              #print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "\n",
    "            if phase == 'val':\n",
    "                val_loss_history.append(epoch_loss)\n",
    "            if phase == 'train':\n",
    "                train_loss_history.append(epoch_loss)\n",
    "            \n",
    "            scheduler.step(epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    return model, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0oMaBzHFU1l7"
   },
   "outputs": [],
   "source": [
    "# put training and validation sets to one dictionary for the use of training function\n",
    "dataloaders_dict = {'train' : trainloader, 'val': validloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mtrOrE4xUEM3"
   },
   "outputs": [],
   "source": [
    "# train model and gather training and validation losses to list\n",
    "model_trained, train_loss_history, val_loss_history = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "x3f5op20jMMx"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model_filename = 'cnn_model.pth'\n",
    "torch.save(model_trained.state_dict(), model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation losses are shown in Figure 5. Model underfits slightly. We tried training also with more epochs (5 000 epochs), but the validation loss started to increase slowly. We decided to cut the training in 500 epochs. For the underfitting problem solution could be to increase the input data or change the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "w2PAfaDNU0qv",
    "outputId": "baba5dfc-9c18-4123-8625-221f21bb2440"
   },
   "outputs": [],
   "source": [
    "# plot training and validation error\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(np.arange(len(train_loss_history)), train_loss_history, label='train')\n",
    "plt.plot(np.arange(len(val_loss_history)), val_loss_history, label='val')\n",
    "plt.ylim([0,2])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error Loss')\n",
    "plt.title('Loss Over Time')\n",
    "plt.legend()\n",
    "plt.savefig('loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"loss.png\" width=800 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 5. Training and validation losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of prediction the test set was divided to equal lenght batches of 48 hours. There was 49 test batches. In the prediction phase input data was fed to model in iterative way. In first iteration last *input_steps* from input data (these hours weren't part of training data, they belonged to validation data) is fed to model and prediction is compared to the first 48 hours of test data. In the next iteration first 48 hours from test data is appended to input data and the prediction is done with this new input data. Mean absolute error (MAE) score is calculated to the prediction. MAE scores for each test batch are shown in Figure 6. It can bee seen that mean absolute error gets bigger when moving away from the split point of training data. In the first 10 batches MAE is less than 10, but after that is increases substantially. This is expectable because model isn't updated between iterations with new input data. The average MAE for all the test batches is 14.03 and for the first 10 batches 5.53.\n",
    "\n",
    "In the Figure 7 there are shown plots of input data, true target data and predictions for the first test batches. It's obvious that the model performace is not very good. The model doesn't capture the high concentration peaks, neither the shape of the variation. We tried also with bigger number of filters but that resulted even worse predictions with huge variation. Model predictions are compared to naive forecast, i.e. last 48 hours are used as a  prediction for next 48 hours. The MAE score of naive prediction is 17.86. Model outperfoms the naive forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XTYRAPq1whT2"
   },
   "outputs": [],
   "source": [
    "model_trained.load_state_dict(torch.load(model_filename, map_location=lambda storage, loc: storage))\n",
    "model_trained.to(torch.device(\"cpu\"))\n",
    "print('Model loaded from %s' % model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bYv5H7OaNAK0"
   },
   "outputs": [],
   "source": [
    "# print weights of second layers convolution\n",
    "model_trained.group[0].conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_r--slb8A-Lw"
   },
   "outputs": [],
   "source": [
    "model_trained.conv2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gkMYtheh_Tz8"
   },
   "outputs": [],
   "source": [
    "# function for predicting next 48 hours values based on \"n_input\" previous time steps using the trained model\n",
    "def predict(model, input_data, n_input, device):\n",
    "    # retrieve last observations for input data\n",
    "    # last observations from input data have not been used in training\n",
    "    inputs = input_data[-n_input:, :]\n",
    "    # Standardize input data\n",
    "    scaler = StandardScaler().fit(inputs)\n",
    "    scaler2 = StandardScaler().fit(inputs[:,0].reshape(-1,1))\n",
    "    inputs = scaler.transform(inputs)\n",
    "    # reshape into [1, channels, n_input]\n",
    "    inputs = inputs.reshape(1, inputs.shape[1], inputs.shape[0])\n",
    "    inputs = torch.tensor(inputs, device=device, dtype=torch.float)\n",
    "    # forecast the next 48 hours\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model.forward(inputs)\n",
    "        outputs = outputs.cpu().data.numpy()\n",
    "        # scale back to original form\n",
    "        outputs_rescaled = scaler2.inverse_transform(outputs.reshape(outputs.shape[2],outputs.shape[1]))\n",
    "    return np.squeeze(outputs_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3X46YmG3YJFd"
   },
   "outputs": [],
   "source": [
    "# function for splitting test data to non-overlapping batches of 48 hours\n",
    "def split_test_data(data, n_out):\n",
    "    y = list()\n",
    "    in_start = 0\n",
    "    for _ in range(len(data)):\n",
    "      # define the end of the sequence\n",
    "      in_end = in_start + n_out\n",
    "      # check that we have enough data for this instance\n",
    "      if in_end < len(data):\n",
    "        batch = data[in_start:in_end]\n",
    "        y.append(batch)\n",
    "        in_start += n_out\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UDyu6ypqJug3",
    "outputId": "5e7fa1b5-6086-4fe1-aa6d-b92866d07b5a"
   },
   "outputs": [],
   "source": [
    "# split test data to batches of 48 hours\n",
    "test_batches = split_test_data(test, output_steps)\n",
    "print(test_batches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uZoXkDAr9pk5"
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(actual, predicted):\n",
    "    scores = list()\n",
    "    # calculate mean absolute error (MAE) score for each 48 hour prediction period\n",
    "    for i in range(actual.shape[0]):\n",
    "        # calculate MAE\n",
    "        mae = mean_absolute_error(actual[i, :], predicted[i, :])\n",
    "        # store\n",
    "        scores.append(mae)\n",
    "        # calculate average MAE\n",
    "        mae_average = sum(scores) / len(scores)\n",
    "        return mae_average, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "t2c1dP4j_NEn"
   },
   "outputs": [],
   "source": [
    "# evaluate model predictions using walk-forward validation\n",
    "# test data is divided to batches of 48 hours and for each batch MAE value is calculated\n",
    "def evaluate_model(model, input_data, test_data, n_input, device):\n",
    "    # walk-forward validation over each 48h batch\n",
    "    predictions = list()\n",
    "    for i in range(len(test_data)):\n",
    "        # predict the 48 hours\n",
    "        yhat = predict(model, input_data, n_input, device)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat)\n",
    "        # get real observation from the test data and add to input_data for predicting the next 48 hours\n",
    "        input_data = np.concatenate([input_data, test_data[i, :, :]])\n",
    "    # evaluate predictions for each 48 hours batch\n",
    "    predictions = np.array(predictions)\n",
    "    mae_average, scores = evaluate_predictions(test_data[:,:,0], predictions)\n",
    "    return mae_average, scores, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "bRhcxK_dN4qE",
    "outputId": "21d0c226-b9bf-4c2c-c6b0-0a3f5bad8807"
   },
   "outputs": [],
   "source": [
    "# calculate MAE values for the mode predictions\n",
    "mae_average, mae_batches, predictions = evaluate_model(model_trained, train, test_batches, input_steps, device)\n",
    "print(mae_average)\n",
    "# MAE for the first 10 batches\n",
    "mae_average_10batch = sum(mae_batches[:10]) / len(mae_batches[:10])\n",
    "print(mae_average_10batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "aKVNYxPblN0d",
    "outputId": "f221b238-1240-47c8-f7eb-156658c1073e"
   },
   "outputs": [],
   "source": [
    "# plot the MAE over different test batches\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(len(mae_batches)), mae_batches)\n",
    "plt.title('MAE over test batches')\n",
    "plt.legend(['MAE'])\n",
    "plt.xlabel('Batch number')\n",
    "plt.ylabel('Mean absolute error')\n",
    "plt.savefig('MAE_batches.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "f6fwz1rKcK7d"
   },
   "outputs": [],
   "source": [
    "# function for plotting predictions and true observations to same figure\n",
    "# figure is created per test batch\n",
    "# modified function, original from https://github.com/JEddy92/TimeSeries_Seq2Seq/blob/master/notebooks/TS_Seq2Seq_Conv_Full.ipynb\n",
    "def plot_predictions(input_data, target_data, predictions, batch_ind, train_tail_len):\n",
    "    input_series = input_data[:,0]\n",
    "    pred_series = predictions[batch_ind,:]\n",
    "    target_series = target_data[batch_ind,:,0]\n",
    "      \n",
    "    input_series_tail = np.concatenate((input_series[-train_tail_len:],target_series[:1]), axis=0) \n",
    "    x = input_series_tail.shape[0]\n",
    "    pred_steps = pred_series.shape[0]\n",
    "      \n",
    "    plt.figure(figsize=(10,6))   \n",
    "      \n",
    "    plt.plot(range(1,x+1),input_series_tail)\n",
    "    plt.plot(range(x,x+pred_steps),target_series,color='orange')\n",
    "    plt.plot(range(x,x+pred_steps),pred_series,color='teal',linestyle='--')\n",
    "      \n",
    "    plt.title('Batch index %d' % batch_ind)\n",
    "    plt.legend(['Input data','Target series','Predictions'])\n",
    "    plt.xlabel('Hours')\n",
    "    plt.ylabel('PM10 concentrations [micro g/m³]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "nr2MVZ1oldtt",
    "outputId": "e0e79366-dc2d-4a07-f70a-63f7312e9222"
   },
   "outputs": [],
   "source": [
    "plot_predictions(train, test_batches, predictions, batch_ind=0, train_tail_len=72)\n",
    "plt.savefig('forecast0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "UP8_xGD-lalf",
    "outputId": "c7441559-a166-43e7-a518-ae54d3a4c74d"
   },
   "outputs": [],
   "source": [
    "plot_predictions(train, test_batches, predictions, batch_ind=1, train_tail_len=72)\n",
    "plt.savefig('forecast1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "39S0Tq1_lfOs",
    "outputId": "8185f825-9e08-485e-8bd1-d9127b15a59b"
   },
   "outputs": [],
   "source": [
    "plot_predictions(train, test_batches, predictions, batch_ind=15, train_tail_len=72)\n",
    "plt.savefig('forecast2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fzJkieQ69uB-"
   },
   "outputs": [],
   "source": [
    "# make naive forecast by taking last 48 hours as the prediction for next 48 hours and calculate MAE\n",
    "def evaluate_naive(input_data, test, n_input):\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # take last 48 hours from training data\n",
    "        value = input_data[-n_input:, 0]\n",
    "        predictions.append(value)\n",
    "        # get real observation from test data and add to input_data\n",
    "        input_data = np.concatenate((value, test[i, :, 0]), axis=0).reshape(-1,1)\n",
    "    # evaluate predictions for each test batches\n",
    "    predictions = np.array(predictions)\n",
    "    mae_average, scores = evaluate_predictions(test[:,:,0], predictions)\n",
    "    return mae_average, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4sYh8-8DF4dn"
   },
   "outputs": [],
   "source": [
    "# calculate MAE values for the naive forecast\n",
    "naive_mae_average, naive_mae_batches = evaluate_naive(train, test_batches, output_steps)\n",
    "print(naive_mae_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zoOIX8M4KgKx"
   },
   "source": [
    "## 4. Conclusions\n",
    "\n",
    "We developed a model for predicting street dust concentrations for 48 hours ahead using past 3 weeks observations. Model is implemented with Pytorch. Model architecture contains stacks of dilated convolutions, residual connections, a SELU activation function and conditioning with multivariate input data.  Input data is hourly observations of PM10 concentrations, amount of rain, temperature and wind speed of 28 months. Input data divided into training, validation and test sets with moving window method, and scaled to zero mean and unit variance before feeding to model. Model outputs directly the 48 hours forecast, and the same model is used for making the predictions for all the test batches.\n",
    "\n",
    "The average MAE of the model for all the test batches is 14.03 and for the first 10 batches 5.53.The MAE score of naive prediction is 17.86. Model outperfoms the naive forecast, but the prediction results are not good. The model doesn't capture the high concentration peaks, neither the overall shape of the concentration curve. The training and validation losses indicates that the model is underfitting slightly. Adding more layers to the model, i.e. expanding the receptive field, could increase the model performance.  Also adding hourly traffic amount as a conditioning data could affect positively to the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XnVlpna8UkQA"
   },
   "source": [
    "## References\n",
    "\n",
    "\n",
    "A. Borovykh, S. Bohte, and C. W. Oosterlee, Conditional Time Series Forecasting with Convolutional Neural Networks, ArXiv e-prints, (2018)\n",
    "\n",
    "A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, WaveNet: A Generative Model for Raw Audio, ArXiv e-prints, (2016)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Street_dust_prediction_with_CNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
